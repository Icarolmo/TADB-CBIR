#!/usr/bin/env python3
"""
Teste R√°pido do Sistema de Avalia√ß√£o CBIR

Este script testa rapidamente se o sistema de avalia√ß√£o est√° funcionando corretamente.
"""

import os
import sys
from pathlib import Path
from evaluation_system import CBIREvaluationSystem
from database import chroma

def test_imports():
    """Testa se todas as depend√™ncias est√£o dispon√≠veis"""
    print("üîç Testando imports...")
    
    try:
        import numpy as np
        print("‚úÖ NumPy importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar NumPy: {e}")
        return False
    
    try:
        import pandas as pd
        print("‚úÖ Pandas importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar Pandas: {e}")
        return False
    
    try:
        from sklearn.metrics import accuracy_score
        print("‚úÖ Scikit-learn importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar Scikit-learn: {e}")
        return False
    
    try:
        import matplotlib.pyplot as plt
        print("‚úÖ Matplotlib importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar Matplotlib: {e}")
        return False
    
    try:
        import seaborn as sns
        print("‚úÖ Seaborn importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar Seaborn: {e}")
        return False
    
    try:
        from evaluation_system import CBIREvaluationSystem
        print("‚úÖ Sistema de avalia√ß√£o importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar sistema de avalia√ß√£o: {e}")
        return False
    
    try:
        from database import chroma
        print("‚úÖ M√≥dulo de banco de dados importado")
    except ImportError as e:
        print(f"‚ùå Erro ao importar banco de dados: {e}")
        return False
    
    return True

def test_evaluation_system():
    """Testa a cria√ß√£o do sistema de avalia√ß√£o"""
    print("\nüîç Testando sistema de avalia√ß√£o...")
    
    try:
        evaluator = CBIREvaluationSystem()
        print("‚úÖ Sistema de avalia√ß√£o criado com sucesso")
        return evaluator
    except Exception as e:
        print(f"‚ùå Erro ao criar sistema de avalia√ß√£o: {e}")
        return None

def test_database_connection():
    """Testa a conex√£o com o banco de dados"""
    print("\nüîç Testando conex√£o com banco de dados...")
    
    try:
        stats = chroma.get_database_stats()
        print(f"‚úÖ Conex√£o com banco estabelecida")
        print(f"   - Total de imagens: {stats['total_images']}")
        print(f"   - Categorias: {stats['categories']}")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao conectar com banco de dados: {e}")
        return False

def test_revocation_prediction():
    """Testa a previs√£o de revoga√ß√£o com dados simulados"""
    print("\nüîç Testando previs√£o de revoga√ß√£o...")
    
    try:
        evaluator = CBIREvaluationSystem()
        
        # Dados simulados para teste
        mock_result = {
            "similar_images": [
                {"similarity": 85.0, "category": "leaf_healthy", "features": {"shape": {"num_lesions": 0, "disease_coverage": 0, "avg_lesion_size": 0}}},
                {"similarity": 82.0, "category": "leaf_healthy", "features": {"shape": {"num_lesions": 0, "disease_coverage": 0, "avg_lesion_size": 0}}},
                {"similarity": 78.0, "category": "leaf_healthy", "features": {"shape": {"num_lesions": 0, "disease_coverage": 0, "avg_lesion_size": 0}}},
                {"similarity": 75.0, "category": "leaf_healthy", "features": {"shape": {"num_lesions": 0, "disease_coverage": 0, "avg_lesion_size": 0}}},
                {"similarity": 70.0, "category": "leaf_healthy", "features": {"shape": {"num_lesions": 0, "disease_coverage": 0, "avg_lesion_size": 0}}}
            ],
            "confidence": 80.0
        }
        
        prediction = evaluator.predict_revocation_risk(mock_result)
        
        print("‚úÖ Previs√£o de revoga√ß√£o funcionando")
        print(f"   - N√≠vel de risco: {prediction['revocation_risk']}")
        print(f"   - Score de risco: {prediction['risk_score']:.3f}")
        print(f"   - Fatores de risco: {len(prediction['risk_factors'])}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro na previs√£o de revoga√ß√£o: {e}")
        return False

def test_metrics_calculation():
    """Testa o c√°lculo de m√©tricas"""
    print("\nüîç Testando c√°lculo de m√©tricas...")
    
    try:
        evaluator = CBIREvaluationSystem()
        
        # Dados simulados para teste
        mock_test_results = [
            {
                "true_category": "leaf_healthy",
                "predicted_category": "leaf_healthy",
                "confidence": 85.0,
                "revocation_risk": "BAIXO",
                "risk_score": 0.2
            },
            {
                "true_category": "leaf_healthy",
                "predicted_category": "leaf_healthy",
                "confidence": 78.0,
                "revocation_risk": "M√âDIO",
                "risk_score": 0.5
            },
            {
                "true_category": "leaf_with_disease",
                "predicted_category": "leaf_with_disease",
                "confidence": 82.0,
                "revocation_risk": "BAIXO",
                "risk_score": 0.3
            },
            {
                "true_category": "leaf_with_disease",
                "predicted_category": "leaf_healthy",
                "confidence": 45.0,
                "revocation_risk": "ALTO",
                "risk_score": 0.8
            }
        ]
        
        metrics = evaluator.calculate_performance_metrics(mock_test_results)
        
        print("‚úÖ C√°lculo de m√©tricas funcionando")
        print(f"   - Acur√°cia: {metrics['overall_accuracy']:.3f}")
        print(f"   - Precis√£o: {metrics['precision']:.3f}")
        print(f"   - Recall: {metrics['recall']:.3f}")
        print(f"   - F1-Score: {metrics['f1_score']:.3f}")
        print(f"   - Confian√ßa m√©dia: {metrics['avg_confidence']:.1f}%")
        print(f"   - Score de risco m√©dio: {metrics['avg_risk_score']:.3f}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no c√°lculo de m√©tricas: {e}")
        return False

def test_directory_structure():
    """Testa se a estrutura de diret√≥rios est√° correta"""
    print("\nüîç Testando estrutura de diret√≥rios...")
    
    required_dirs = [
        "image/dataset",
        "image/test_dataset", 
        "image/uploads",
        "database"
    ]
    
    missing_dirs = []
    for dir_path in required_dirs:
        if not os.path.exists(dir_path):
            missing_dirs.append(dir_path)
        else:
            print(f"‚úÖ {dir_path} existe")
    
    if missing_dirs:
        print(f"‚ö†Ô∏è Diret√≥rios faltando: {missing_dirs}")
        print("Execute os seguintes comandos para criar:")
        for dir_path in missing_dirs:
            print(f"   mkdir -p {dir_path}")
        return False
    
    return True

def test_gui_import():
    """Testa se a interface gr√°fica pode ser importada"""
    print("\nüîç Testando interface gr√°fica...")
    
    try:
        import tkinter as tk
        print("‚úÖ Tkinter dispon√≠vel")
        
        # Testar se matplotlib backend funciona
        import matplotlib
        matplotlib.use('TkAgg')
        print("‚úÖ Matplotlib backend configurado")
        
        return True
    except ImportError as e:
        print(f"‚ùå Erro ao importar interface gr√°fica: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Erro na configura√ß√£o da interface: {e}")
        return False

def main():
    """Fun√ß√£o principal do teste"""
    print("üß™ TESTE R√ÅPIDO DO SISTEMA DE AVALIA√á√ÉO CBIR")
    print("=" * 50)
    
    tests = [
        ("Imports", test_imports),
        ("Sistema de avalia√ß√£o", test_evaluation_system),
        ("Conex√£o com banco", test_database_connection),
        ("Previs√£o de revoga√ß√£o", test_revocation_prediction),
        ("C√°lculo de m√©tricas", test_metrics_calculation),
        ("Estrutura de diret√≥rios", test_directory_structure),
        ("Interface gr√°fica", test_gui_import)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"‚ùå Erro inesperado em {test_name}: {e}")
    
    print("\n" + "=" * 50)
    print(f"üìä RESULTADO DOS TESTES: {passed}/{total} passaram")
    
    if passed == total:
        print("üéâ Todos os testes passaram! O sistema est√° funcionando corretamente.")
        print("\nüìñ Pr√≥ximos passos:")
        print("1. Processe um dataset: python cbir.py --process-only")
        print("2. Teste a previs√£o: python cbir.py")
        print("3. Execute avalia√ß√£o: python cbir.py --evaluate")
        print("4. Use a interface: python evaluation_gui.py")
        print("5. Veja a demonstra√ß√£o: python demo_evaluation.py")
    else:
        print("‚ö†Ô∏è Alguns testes falharam. Verifique as depend√™ncias e configura√ß√£o.")
        print("\nüîß Solu√ß√µes comuns:")
        print("1. Instale depend√™ncias: pip install -r requirements.txt")
        print("2. Crie diret√≥rios: mkdir -p image/dataset image/test_dataset image/uploads")
        print("3. Verifique se o banco de dados est√° configurado")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 